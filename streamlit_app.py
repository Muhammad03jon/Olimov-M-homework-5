import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from mlxtend.plotting import plot_decision_regions

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.title('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–π –∏–ª–∏ —Ñ–∞–ª—å—à–∏–≤–æ–π –±–∞–Ω–∫–Ω–æ—Ç—ã')

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
file_path = "https://raw.githubusercontent.com/Muhammad03jon/Olimov-M-homework-5/refs/heads/master/data_banknote_authentication.txt"
df = pd.read_csv(file_path, sep=",", header=None)
df.columns = ["variance", "skewness", "curtosis", "entropy", "class"]

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
with st.expander('–î–∞–Ω–Ω—ã–µ'):
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("X (–ü—Ä–∏–∑–Ω–∞–∫–∏)")
        X_raw = df.drop('class', axis=1)
        st.dataframe(X_raw.style.set_properties(**{'background-color': '#f0f2f6', 'color': 'black'}))

    with col2:
        st.subheader("y (–¶–µ–ª—å)")
        y_raw = df['class']
        st.dataframe(y_raw.to_frame().style.set_properties(**{'background-color': '#e8f4ea', 'color': 'black'}))

# –í–≤–æ–¥ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ –±–æ–∫–æ–≤—É—é –ø–∞–Ω–µ–ª—å
with st.sidebar:
    st.header("–í–≤–µ–¥–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
    
    # –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞
    use_random_sample = st.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü")

    if use_random_sample:
        random_sample = df.sample(1).iloc[0]
        variance = random_sample["variance"]
        skewness = random_sample["skewness"]
        curtosis = random_sample["curtosis"]
        entropy = random_sample["entropy"]
        st.write("–í—ã–±—Ä–∞–Ω—ã —Å–ª—É—á–∞–π–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:")
    else:
        # –í–≤–æ–¥ –∑–Ω–∞—á–µ–Ω–∏–π —á–µ—Ä–µ–∑ —Å–ª–∞–π–¥–µ—Ä—ã
        variance = st.slider('Variance', float(df["variance"].min()), float(df["variance"].max()), float(df["variance"].mean()))
        skewness = st.slider('Skewness', float(df["skewness"].min()), float(df["skewness"].max()), float(df["skewness"].mean()))
        curtosis = st.slider('Curtosis', float(df["curtosis"].min()), float(df["curtosis"].max()), float(df["curtosis"].mean()))
        entropy = st.slider('Entropy', float(df["entropy"].min()), float(df["entropy"].max()), float(df["entropy"].mean()))

    data = {
        "variance": variance,
        "skewness": skewness,
        "curtosis": curtosis,
        "entropy": entropy
    }

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    st.write(f"–í—ã–±—Ä–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:\n- Variance: {variance}\n- Skewness: {skewness}\n- Curtosis: {curtosis}\n- Entropy: {entropy}")

st.subheader("üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö")

# –ì—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
st.subheader("–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

fig, axes = plt.subplots(4, 2, figsize=(12, 20))

for i, col in enumerate(["variance", "skewness", "curtosis", "entropy"]):
    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞
    ax_hist = axes[i, 0]
    sns.histplot(df[col], ax=ax_hist, bins=30, kde=False, color='skyblue', alpha=0.6)
    ax_hist.set_title(f"–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞: {col}")
    ax_hist.set_xlabel(col)
    ax_hist.set_ylabel("–ß–∞—Å—Ç–æ—Ç–∞")

    # –ì—Ä–∞—Ñ–∏–∫ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
    ax_kde = axes[i, 1]
    sns.kdeplot(data=df, x=col, hue='class', fill=True, ax=ax_kde, palette='Set1', alpha=0.5)
    ax_kde.set_title(f"–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: {col} –ø–æ –∫–ª–∞—Å—Å–∞–º")
    ax_kde.set_xlabel(col)
    ax_kde.set_ylabel("–ü–ª–æ—Ç–Ω–æ—Å—Ç—å")

# –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ—Ç—Å—Ç—É–ø—ã –º–µ–∂–¥—É –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
plt.subplots_adjust(wspace=0.4, hspace=0.4)
st.pyplot(fig)

# Train test split
X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.3, random_state=42)

# StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_train.columns)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_scaled, y_train)

log_reg = LogisticRegression(max_iter=565)
log_reg.fit(X_train_scaled, y_train)

d_tree = DecisionTreeClassifier(max_depth=5)
d_tree.fit(X_train_scaled, y_train)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–Ω–∏—Ü —Ä–µ—à–µ–Ω–∏–π
X_array = X_train_scaled.to_numpy() 
y_array = y_train.to_numpy()

classifiers = [knn, log_reg, d_tree]
titles = ['KNeighborsClassifier', 'Logistic Regression', 'Decision Tree']

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for ax, clf, title in zip(axes, classifiers, titles):
    plot_decision_regions(X_array, y_array, clf=clf, ax=ax)
    ax.set_title(title)
    ax.set_xlabel('variance')
    ax.set_ylabel('skewness')

plt.tight_layout()
st.pyplot(fig)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
if st.button("–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å"):
    input_data = np.array([[variance, skewness, curtosis, entropy]])
    input_scaled = scaler.transform(input_data)  # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –∫–∞–∂–¥–æ–π –∏–∑ –º–æ–¥–µ–ª–µ–π
    prediction_knn = knn.predict(input_scaled)[0]
    prediction_log_reg = log_reg.predict(input_scaled)[0]
    prediction_dtree = d_tree.predict(input_scaled)[0]

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:")
    st.write(f"KNeighborsClassifier: {'–§–∞–ª—å—à–∏–≤–∞—è' if prediction_knn == 0 else '–ù–∞—Å—Ç–æ—è—â–∞—è'} –±–∞–Ω–∫–Ω–æ—Ç–∞")
    st.write(f"Logistic Regression: {'–§–∞–ª—å—à–∏–≤–∞—è' if prediction_log_reg == 0 else '–ù–∞—Å—Ç–æ—è—â–∞—è'} –±–∞–Ω–∫–Ω–æ—Ç–∞")
    st.write(f"Decision Tree: {'–§–∞–ª—å—à–∏–≤–∞—è' if prediction_dtree == 0 else '–ù–∞—Å—Ç–æ—è—â–∞—è'} –±–∞–Ω–∫–Ω–æ—Ç–∞")
