import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
from mlxtend.plotting import plot_decision_regions

st.title('üîç –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–π –∏–ª–∏ —Ñ–∞–ª—å—à–∏–≤–æ–π –±–∞–Ω–∫–Ω–æ—Ç—ã')

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
file_path = "https://raw.githubusercontent.com/Muhammad03jon/Olimov-M-homework-5/refs/heads/master/data_banknote_authentication.txt"
df = pd.read_csv(file_path, sep=",", header=None)
df.columns = ["variance", "skewness", "curtosis", "entropy", "class"]

st.subheader('üìä –î–∞–Ω–Ω—ã–µ')
st.dataframe(df.head())

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
X_raw = df.drop('class', axis=1)
y_raw = df['class']

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.3, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
st.sidebar.header("‚öôÔ∏è –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
model_choice = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å", ["KNN", "Logistic Regression", "Decision Tree"])

if model_choice == "KNN":
    n_neighbors = st.sidebar.slider("–ß–∏—Å–ª–æ —Å–æ—Å–µ–¥–µ–π", 1, 20, 3)
    model = KNeighborsClassifier(n_neighbors=n_neighbors)
elif model_choice == "Logistic Regression":
    max_iter = st.sidebar.slider("–ß–∏—Å–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π", 100, 1000, 500)
    model = LogisticRegression(max_iter=max_iter)
elif model_choice == "Decision Tree":
    max_depth = st.sidebar.slider("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞", 1, 20, 5)
    model = DecisionTreeClassifier(max_depth=max_depth)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model.fit(X_train_scaled, y_train)
y_pred = model.predict(X_test_scaled)

# –í—ã–±–æ—Ä–∫–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
st.sidebar.header("üî¢ –í–≤–µ–¥–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
variance = st.sidebar.slider("Variance", float(df["variance"].min()), float(df["variance"].max()), float(df["variance"].mean()))
skewness = st.sidebar.slider("Skewness", float(df["skewness"].min()), float(df["skewness"].max()), float(df["skewness"].mean()))
curtosis = st.sidebar.slider("Curtosis", float(df["curtosis"].min()), float(df["curtosis"].max()), float(df["curtosis"].mean()))
entropy = st.sidebar.slider("Entropy", float(df["entropy"].min()), float(df["entropy"].max()), float(df["entropy"].mean()))

sample = np.array([[variance, skewness, curtosis, entropy]])
sample_scaled = scaler.transform(sample)
prediction = model.predict(sample_scaled)
st.sidebar.write("**–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:** ", "–ù–∞—Å—Ç–æ—è—â–∞—è –±–∞–Ω–∫–Ω–æ—Ç–∞" if prediction[0] == 0 else "–§–∞–ª—å—à–∏–≤–∞—è –±–∞–Ω–∫–Ω–æ—Ç–∞")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
st.subheader("üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏")
fig, ax = plt.subplots(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', ax=ax)
st.pyplot(fig)

st.text("–û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
st.text(classification_report(y_test, y_pred))

# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
if model_choice == "Decision Tree":
    st.subheader("üî¨ –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    feature_importances = model.feature_importances_
    fig, ax = plt.subplots()
    sns.barplot(x=X_raw.columns, y=feature_importances, ax=ax)
    ax.set_title("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    st.pyplot(fig)

# –ó–∞–≥—Ä—É–∑–∫–∞ CSV
st.subheader("üì• –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ CSV-—Ñ–∞–π–ª", type=["csv"])
if uploaded_file:
    new_data = pd.read_csv(uploaded_file)
    new_data_scaled = scaler.transform(new_data)
    predictions = model.predict(new_data_scaled)
    st.write("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:")
    st.write(predictions)
