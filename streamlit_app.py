import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from mlxtend.plotting import plot_decision_regions

st.title('üíµ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–π –∏–ª–∏ —Ñ–∞–ª—å—à–∏–≤–æ–π –±–∞–Ω–∫–Ω–æ—Ç—ã')

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
file_path = "https://raw.githubusercontent.com/Muhammad03jon/Olimov-M-homework-5/refs/heads/master/data_banknote_authentication.txt"
df = pd.read_csv(file_path, sep=",", header=None)
df.columns = ["variance", "skewness", "curtosis", "entropy", "class"]

with st.expander('üìÇ –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ'):
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("X (–ü—Ä–∏–∑–Ω–∞–∫–∏)")
        X_raw = df.drop('class', axis=1)
        st.dataframe(X_raw)
    with col2:
        st.subheader("y (–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)")
        y_raw = df['class']
        st.dataframe(y_raw.to_frame())

# –í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
with st.sidebar:
    st.header("üéõ –í–≤–µ–¥–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ")
    use_random_sample = st.checkbox("üìå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü")
    if use_random_sample:
        random_sample = df.sample(1).iloc[0]
        variance, skewness, curtosis, entropy = random_sample[:4]
    else:
        variance = st.slider('Variance', float(df["variance"].min()), float(df["variance"].max()), float(df["variance"].mean()))
        skewness = st.slider('Skewness', float(df["skewness"].min()), float(df["skewness"].max()), float(df["skewness"].mean()))
        curtosis = st.slider('Curtosis', float(df["curtosis"].min()), float(df["curtosis"].max()), float(df["curtosis"].mean()))
        entropy = st.slider('Entropy', float(df["entropy"].min()), float(df["entropy"].max()), float(df["entropy"].mean()))
    
    data = {"variance": variance, "skewness": skewness, "curtosis": curtosis, "entropy": entropy}
    st.write("**–í—ã–±—Ä–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**", data)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
st.subheader("üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö")
fig, axes = plt.subplots(4, 2, figsize=(12, 20))
for i, col in enumerate(["variance", "skewness", "curtosis", "entropy"]):
    sns.histplot(df[col], ax=axes[i, 0], bins=30, kde=False, color='skyblue')
    axes[i, 0].set_title(f"–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞: {col}")
    sns.kdeplot(data=df, x=col, hue='class', fill=True, ax=axes[i, 1], palette='Set1', alpha=0.5)
    axes[i, 1].set_title(f"–ü–ª–æ—Ç–Ω–æ—Å—Ç—å: {col} –ø–æ –∫–ª–∞—Å—Å–∞–º")
plt.subplots_adjust(wspace=0.4, hspace=0.4)
st.pyplot(fig)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.3, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_train.columns)
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_scaled, y_train)
log_reg = LogisticRegression(max_iter=565)
log_reg.fit(X_train_scaled, y_train)
d_tree = DecisionTreeClassifier(max_depth=5)
d_tree.fit(X_train_scaled, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
sample_df = pd.DataFrame([data])
sample_scaled = scaler.transform(sample_df)  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ

knn_pred = knn.predict(sample_scaled)[0]
log_reg_pred = log_reg.predict(sample_scaled)[0]
d_tree_pred = d_tree.predict(sample_scaled)[0]

st.subheader("üîÆ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
col1, col2, col3 = st.columns(3)
col1.metric("KNN", "–†–µ–∞–ª—å–Ω–∞—è" if knn_pred == 0 else "–§–∞–ª—å—à–∏–≤–∞—è")
col2.metric("Logistic Regression", "–†–µ–∞–ª—å–Ω–∞—è" if log_reg_pred == 0 else "–§–∞–ª—å—à–∏–≤–∞—è")
col3.metric("Decision Tree", "–†–µ–∞–ª—å–Ω–∞—è" if d_tree_pred == 0 else "–§–∞–ª—å—à–∏–≤–∞—è")

# –ì—Ä–∞–Ω–∏—Ü—ã —Ä–µ—à–µ–Ω–∏–π
st.subheader("üìç –ì—Ä–∞–Ω–∏—Ü—ã —Ä–µ—à–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π")
X_array, y_array = X_train_scaled.iloc[:, :2].to_numpy(), y_train.to_numpy()
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
for ax, clf, title in zip(axes, [knn, log_reg, d_tree], ['KNN', 'Logistic Regression', 'Decision Tree']):
    plot_decision_regions(X_array, y_array, clf=clf, ax=ax)
    ax.set_title(title)
    ax.set_xlabel('variance')
    ax.set_ylabel('skewness')
plt.tight_layout()
st.pyplot(fig)
