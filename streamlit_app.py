import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, classification_report

st.title('üíµ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–π –∏–ª–∏ —Ñ–∞–ª—å—à–∏–≤–æ–π –±–∞–Ω–∫–Ω–æ—Ç—ã')

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
file_path = "https://raw.githubusercontent.com/Muhammad03jon/Olimov-M-homework-5/refs/heads/master/data_banknote_authentication.txt"
df = pd.read_csv(file_path, sep=",", header=None)
df.columns = ["variance", "skewness", "curtosis", "entropy", "class"]

with st.expander('üìÇ –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ'):
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("X (–ü—Ä–∏–∑–Ω–∞–∫–∏)")
        X_raw = df.drop('class', axis=1)
        st.dataframe(X_raw)
    with col2:
        st.subheader("y (–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)")
        y_raw = df['class']
        st.dataframe(y_raw.to_frame())

# –í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
with st.sidebar:
    st.header("üéõ –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    model_choice = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:", ["KNN", "–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è", "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π"])

    if model_choice == "KNN":
        n_neighbors = st.slider('n_neighbors', 1, 20, 3)
    elif model_choice == "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π":
        max_depth = st.slider('max_depth', 1, 10, 5)

    use_random_sample = st.checkbox("üìå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü")
    if use_random_sample:
        random_sample = df.sample(1).iloc[0]
        variance, skewness, curtosis, entropy = random_sample[:4]
    else:
        variance = st.slider('Variance', float(df["variance"].min()), float(df["variance"].max()), float(df["variance"].mean()))
        skewness = st.slider('Skewness', float(df["skewness"].min()), float(df["skewness"].max()), float(df["skewness"].mean()))
        curtosis = st.slider('Curtosis', float(df["curtosis"].min()), float(df["curtosis"].max()), float(df["curtosis"].mean()))
        entropy = st.slider('Entropy', float(df["entropy"].min()), float(df["entropy"].max()), float(df["entropy"].mean()))

    data = {"variance": variance, "skewness": skewness, "curtosis": curtosis, "entropy": entropy}
    st.write("**–í—ã–±—Ä–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**", data)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
st.subheader("üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö")
fig, axes = plt.subplots(4, 2, figsize=(12, 20))
for i, col in enumerate(["variance", "skewness", "curtosis", "entropy"]):
    sns.histplot(df[col], ax=axes[i, 0], bins=30, kde=False, color='skyblue')
    axes[i, 0].set_title(f"–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞: {col}")
    sns.kdeplot(data=df, x=col, hue='class', fill=True, ax=axes[i, 1], palette='Set1', alpha=0.5)
    axes[i, 1].set_title(f"–ü–ª–æ—Ç–Ω–æ—Å—Ç—å: {col} –ø–æ –∫–ª–∞—Å—Å–∞–º")
plt.subplots_adjust(wspace=0.4, hspace=0.4)
st.pyplot(fig)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.3, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
if model_choice == "KNN":
    model = KNeighborsClassifier(n_neighbors=n_neighbors)
elif model_choice == "–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è":
    model = LogisticRegression(max_iter=565)
else:  # –î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π
    model = DecisionTreeClassifier(max_depth=max_depth)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model.fit(X_train_scaled, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
sample_df = pd.DataFrame([data])
sample_scaled = scaler.transform(sample_df)

prediction = model.predict(sample_scaled)[0]

st.subheader("üîÆ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
st.metric("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ", "–†–µ–∞–ª—å–Ω–∞—è" if prediction == 0 else "–§–∞–ª—å—à–∏–≤–∞—è")

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
st.header("üì• –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Ñ–∞–π–ª —Å –Ω–æ–≤—ã–º–∏ –±–∞–Ω–∫–Ω–æ—Ç–∞–º–∏", type="csv")
if uploaded_file is not None:
    new_data = pd.read_csv(uploaded_file)
    new_data_scaled = scaler.transform(new_data)
    new_predictions = model.predict(new_data_scaled)
    st.write("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    st.dataframe(pd.DataFrame(new_predictions, columns=["Prediction"]))

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π
if model_choice == "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π":
    importance = model.feature_importances_
    feature_names = X_raw.columns
    fig, ax = plt.subplots()
    ax.barh(feature_names, importance)
    ax.set_xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
    ax.set_title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π')
    st.pyplot(fig)

# –ì—Ä–∞—Ñ–∏–∫–∏ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π
if st.button("–ü–æ–∫–∞–∑–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"):
    y_pred = model.predict(X_test_scaled)
    st.subheader("üìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏")
    st.write("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
    conf_matrix = confusion_matrix(y_test, y_pred)
    st.write(conf_matrix)

    report = classification_report(y_test, y_pred, output_dict=True)
    st.write("–û—Ç—á–µ—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º:")
    st.write(f"Precision: {report['0']['precision']:.2f}, Recall: {report['0']['recall']:.2f}, F1-score: {report['0']['f1-score']:.2f}")
