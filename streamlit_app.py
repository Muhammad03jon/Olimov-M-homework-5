import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc

st.title('üíµ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–π –∏–ª–∏ —Ñ–∞–ª—å—à–∏–≤–æ–π –±–∞–Ω–∫–Ω–æ—Ç—ã')

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
file_path = "https://raw.githubusercontent.com/Muhammad03jon/Olimov-M-homework-5/refs/heads/master/data_banknote_authentication.txt"
df = pd.read_csv(file_path, sep=",", header=None)
df.columns = ["variance", "skewness", "curtosis", "entropy", "class"]

with st.expander('–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ'):
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("X (–ü—Ä–∏–∑–Ω–∞–∫–∏)")
        X_raw = df.drop('class', axis=1)
        st.dataframe(X_raw)
    with col2:
        st.subheader("y (–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)")
        y_raw = df['class']
        st.dataframe(y_raw.to_frame())

# –í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö
with st.sidebar:
    st.header("–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    model_choice = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:", ["KNN", "–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è", "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π"])

    if model_choice == "KNN":
        n_neighbors = st.slider('n_neighbors', 1, 20, 3)
    elif model_choice == "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π":
        max_depth = st.slider('max_depth', 1, 10, 5)
    elif model_choice == "–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è":
        solver = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:", ["liblinear", "lbfgs"])
        C = st.slider('C (–æ–±—Ä–∞—Ç–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)', 0.01, 10.0, 1.0)

    use_random_sample = st.checkbox("üìå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü")
    if use_random_sample:
        random_sample = df.sample(1).iloc[0]
        variance, skewness, curtosis, entropy = random_sample[:4]
    else:
        variance = st.slider('Variance', float(df["variance"].min()), float(df["variance"].max()), float(df["variance"].mean()))
        skewness = st.slider('Skewness', float(df["skewness"].min()), float(df["skewness"].max()), float(df["skewness"].mean()))
        curtosis = st.slider('Curtosis', float(df["curtosis"].min()), float(df["curtosis"].max()), float(df["curtosis"].mean()))
        entropy = st.slider('Entropy', float(df["entropy"].min()), float(df["entropy"].max()), float(df["entropy"].mean()))

    st.write("**–í—ã–±—Ä–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**")
    st.write(f"Variance: {variance}")
    st.write(f"Skewness: {skewness}")
    st.write(f"Curtosis: {curtosis}")
    st.write(f"Entropy: {entropy}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
st.subheader("üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö")
fig, axes = plt.subplots(4, 2, figsize=(12, 20))
for i, col in enumerate(["variance", "skewness", "curtosis", "entropy"]):
    sns.histplot(df[col], ax=axes[i, 0], bins=30, kde=False, color='skyblue')
    axes[i, 0].set_title(f"–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞: {col}")
    sns.kdeplot(data=df, x=col, hue='class', fill=True, ax=axes[i, 1], palette='Set1', alpha=0.5)
    axes[i, 1].set_title(f"–ü–ª–æ—Ç–Ω–æ—Å—Ç—å: {col} –ø–æ –∫–ª–∞—Å—Å–∞–º")
plt.subplots_adjust(wspace=0.4, hspace=0.4)
st.pyplot(fig)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.3, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#  –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π
if model_choice == "KNN":
    model = KNeighborsClassifier(n_neighbors=n_neighbors)
elif model_choice == "–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è":
    model = LogisticRegression(solver=solver, C=C, max_iter=565)
elif model_choice == "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π":
    model = DecisionTreeClassifier(max_depth=max_depth)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model.fit(X_train_scaled, y_train)

# –ö–Ω–æ–ø–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
if st.button("–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å"):
    data = {"variance": variance, "skewness": skewness, "curtosis": curtosis, "entropy": entropy}
    sample_df = pd.DataFrame([data])
    sample_scaled = scaler.transform(sample_df)

    prediction = model.predict(sample_scaled)[0]

    st.subheader("üîÆ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
    st.metric("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ", "–†–µ–∞–ª—å–Ω–∞—è" if prediction == 0 else "–§–∞–ª—å—à–∏–≤–∞—è")

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
st.header("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Ñ–∞–π–ª —Å –Ω–æ–≤—ã–º–∏ –±–∞–Ω–∫–Ω–æ—Ç–∞–º–∏", type="csv")
if uploaded_file is not None:
    new_data = pd.read_csv(uploaded_file)
    new_data_scaled = scaler.transform(new_data)
    new_predictions = model.predict(new_data_scaled)
    st.write("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    st.dataframe(pd.DataFrame(new_predictions, columns=["Prediction"]))

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π
if model_choice == "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π":
    importance = model.feature_importances_
    fig, ax = plt.subplots()
    ax.barh(X_raw.columns, importance)
    ax.set_xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
    ax.set_title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π')
    st.pyplot(fig)

# –ì—Ä–∞—Ñ–∏–∫–∏ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π
if st.button("–ü–æ–∫–∞–∑–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"):
    y_pred_train = model.predict(X_train_scaled)
    y_pred_test = model.predict(X_test_scaled)

    # Accuracy
    accuracy_train = (y_pred_train == y_train).mean()
    accuracy_test = (y_pred_test == y_test).mean()
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    report_train = classification_report(y_train, y_pred_train, output_dict=True)
    report_test = classification_report(y_test, y_pred_test, output_dict=True)

    st.subheader("üìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏")
    
    # –¢–∞–±–ª–∏—Ü–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    metrics_df = pd.DataFrame({
        'Metric': ['Precision (Train)', 'Recall (Train)', 'F1-score (Train)', 'Accuracy (Train)',
                   'Precision (Test)', 'Recall (Test)', 'F1-score (Test)', 'Accuracy (Test)'],
        'Value': [
            report_train['1']['precision'], report_train['1']['recall'], report_train['1']['f1-score'], accuracy_train,
            report_test['1']['precision'], report_test['1']['recall'], report_test['1']['f1-score'], accuracy_test
        ]
    })
    st.table(metrics_df)

    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    conf_matrix = confusion_matrix(y_test, y_pred_test)
    fig, ax = plt.subplots()
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax)
    ax.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ')
    ax.set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
    ax.set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')
    st.pyplot(fig)

    # ROC AUC
    y_scores = model.predict_proba(X_test_scaled)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_test, y_scores)
    roc_auc = auc(fpr, tpr)

    fig, ax = plt.subplots()
    ax.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')
    ax.plot([0, 1], [0, 1], 'k--')
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title('Receiver Operating Characteristic (ROC)')
    ax.legend(loc="lower right")
    st.pyplot(fig)
